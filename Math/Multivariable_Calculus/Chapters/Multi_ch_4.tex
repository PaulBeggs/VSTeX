\section*{4.1-4.3 Functions of Several Variables}
\setcounter{section}{1}
A \textit{scalar function} is a function \(f \colon \R^{n} \to \R\). This is in some sense a reverse of a parametric function. Parametric functions take in a single real variable \(t\) and output an answer of multiple variable (which we typically interpreted as a vector). A scalar function takes in multiple independent variables (typically interpreted as a point in \(\R^{n}\)) and outputs a single real variable.

\subsubsection{Notation and Graphs}

When \(f \colon \R^{2} \to \R\), we can write \(z = f(x, y)\). For each point \((x, y)\) in the domain of \(f\), we can interpret \(z\) as a height, so that the set of points \((x, y, z)\) which satisfy \(z = f(x, y)\) form a surface in \(\R^{3}\). You can graph this in GeoGebra using \url{geogebra.com/3d}. \\

When \(f \colon \R^{3} \to \R\), we often write \(w = f(x, y, z)\). Then, the point \((x, y, z)\) is in the domain and \(w\) is still a height, though since the graph lives we are in \(\R^{4}\), we cannot draw it in a simple way.

\subsubsection{Contour Plots}

Let \(f \colon \R^{2} \to \R\) be a function. The graph is a surface in \(\R^{3}\), but often it can be difficult to show this on paper. For a given \(z_{0} \in \R\) a \textit{level curve} consists of all points \((x, y) \in \R^{2}\) for which \(f(x, y) = z_{0}\). If we think of the function \(f\) as describing a height at a function of two-dimension position, a level curve is the set of all points with a particular given height. \\

A \textit{contour plot} consists of a collection of level curves – typically, we select some number of equally spaced \(\{z_{1}, z_{2}, \ldots, z_{n}\}\) and build the level curves for each. This gives a “topographical map” of the function \(f\).

\subsection{Limits}

\subsubsection{Formal Definition of \(\lim_{(x, y) \to (a, b)} f(x, y)\)}

Let \(f \colon \R^{2} \to \R\), \((a, b) \in \R^{2}\), and \(L \in \R\). The statement that \(f\) has limit \(L\) as \((x, y)\) approaches \((a, b)\) means that for each choice of \(\epsilon > 0\), there exists \(\delta > 0\) such that for all \((x, y) \neq (a, b)\) within a disk of radius \(\delta\), centered at \((a, b)\), then

\[
|f(x, y) - L| < \epsilon.
\]

Essentially, this says that the limit exists whenever we can make the output from \(f\) as close to \(L\) as we’d like by choosing \((x, y)\) sufficiently close to \((a, b)\). \\

In Calculus I, you showed that a limit \(\lim_{x \to a} f(x)\) exists if and only if the left and right hand limits both exist and match. Basically, you showed that no matter if you approach \(a\) from the left or from the right, you always get the same answer. The same idea applies here, but there is an important complication: We need to check every possible path into the point \((a, b)\) – and rather than just two, there are infinitely many paths! \\

In general, there is not a simple recipe that will work. You can try various combinations of paths until something goes wrong. For practice set/exam purposes, I will always tell you that the limit does not exist, and ask you to prove it. Showing that a limit does exist is a completely different procedure – just because two paths happen to produce the same answer does not prove that all do. \\

Problem \#4 on Practice Set \#3 will ask you to show a limit exists. It provides step-by-step instructions and this is the only time I will ask you to do this.

\subsubsection{Limit Laws}

The basic rules are all the same. Suppose that \(a, b, c \in \R\) and \(f, g \colon \R^{2} \to \R\) and that \(\lim_{(x, y) \to (a, b)} f(x, y) = L\) and \(\lim_{(x, y) \to (a, b)} g(x, y) = M\).

\begin{multicols}{2}
    \begin{itemize}
        \item \(\lim_{(x, y) \to (a, b)} c = c\)
        \item \(\lim_{(x, y) \to (a, b)} x = a\)
        \item \(\lim_{(x, y) \to (a, b)} y = b\)
        \item \(\lim_{(x, y) \to (a, b)} [f(x, y) + g(x, y)] = L + M\)
        \item \(\lim_{(x, y) \to (a, b)} [cf(x, y)] = cL\)
    \end{itemize}
\end{multicols}
\vspace*{-0.5cm}
\begin{itemize}
    \item if \(f\) has a single, simple algebraic definition and \((a, b)\) is in the domain, then\\ 
    \(\lim_{(x, y) \to (a, b)} f(x, y) = f(a, b)\).
\end{itemize}

\subsection{Continuity}

A function \(f \colon \R^{2} \to \R\) is continuous at \((a, b) \in \R^{2}\) provided that \(\lim_{(x, y) \to (a, b)} f(x, y)\) exists and is equal in value to \(f(a, b)\). This is exactly our usual definition of continuity from Calculus I again – and there is nothing special here about being in two dimensions. \\

All of the usual algebraic functions (polynomials, root functions, trig functions, exponentials, and logarithms) and their combinations by addition, subtraction, multiplication, division, exponentiation, and composition are continuous everywhere in their domains.

\subsection{Partial Derivatives}

Suppose \(f \colon \R^{n} \to \R\) is a scalar function. Then, the \cyanit{partial derivative} of \(f\) with respect to \(x_{i}\) is
\[
\dfrac{\partial f}{\partial x_{i}} = \lim_{h \to 0} \dfrac{f(x_{1}, x_{2}, \ldots, x_{i} + h, \ldots, x_{n}) - f(x_{1}, x_{2}, \ldots, x_{i}, \ldots, x_{n})}{h},
\]
if this limit exists. \\

That is, we hold all other variables constant and take the derivative as though \(x_{i}\) is the only variable. Just like in Calculus I, the partial derivative is a rate of change – it measures the ratio of how much the output of \(f\) changes relative to a small change in \(x_{i}\).

\subsubsection{Notation}

Suppose that \(f\) is a function and \(x\) is one of its variables. Then, we denote the partial derivative of \(f\) with respect to \(x\) by any of:
\begin{itemize}
    \item Leibnitz/Condorcet Notation: \(\dfrac{\partial f}{\partial x}\)
    \item Jacobi Notation: \(f_{x}\)
    \item Euler Operation Notation: \(D_{x}f\)
\end{itemize}
Higher order derivatives are notated as:
\begin{itemize}
    \item Leibnitz/Condorcet Notation: \(\dfrac{\partial^{n} f}{\partial x^{n}}\)
    \item Jacobi Notation: \(f_{xxx\ldots x}\)
    \item Euler Operation Notation: \(D_{n}f\)
\end{itemize}

\subsection{Mixed Partials}

Given a function \(f \colon \R^{2} \to \R\) we can determine the \cyanit{mixed partial derivative} of \(f\) with respect to \(x\) and then \(y\). In Leibnitz notation, this would be
\[
\dfrac{\partial}{\partial y} \left( \dfrac{\partial f}{\partial x} \right) \quad \text{or in Jacobi notation} \quad f_{xy}.
\]

\subsubsection{Clairaut’s Theorem}

Suppose that \(f \colon \R^{2} \to \R\) is defined on an open disk \(D\) containing the point \((a, b)\). If each of \(f_{xy}\) and \(f_{yx}\) are continuous on \(D\), then \(f_{xy}(a, b) = f_{yx}(a, b)\). \\

That is, for pretty much any function you’ll even encounter, order does not matter for the mixed partial derivatives! This is especially nice, since notationally, \(\dfrac{\partial^{2} f}{\partial x \partial y} = f_{yx}\) and \(\dfrac{\partial^{2} f}{\partial y \partial x} = f_{xy}\), and who wants to worry about keeping up with that!

\newpage

\setcounter{section}{3}

\section{Tangent Planes}

\subsection{Tangent Lines}

For a single-variable differentiable function \(f \colon \R \to \R\), in Calculus I we defined the \cyanit{tangent line} at \(x = x_{0}\) as the unique line
\[
y = f'(x_{0})(x - x_{0}) + f(x_{0}).
\]
This is the line which most closely matches the graph of \(y = f(x)\) at the desired point. Functions which have tangent lines are said to be “smooth” or “locally linear.”

\subsection{Tangent Planes}

For functions \(f \colon \R^{2} \to \R\) we have a similar idea. If the surface generated by such a function has no sharp corners or edges, you might see that as you zoom in, the surface becomes flatter and flatter – and will eventually resemble a plane. In fact, we define the \cyanit{tangent plane} as the unique plane at \((x, y) = (x_{0}, y_{0})\) which satisfies
\[
z = f(x_{0}, y_{0}) + f_{x}(x_{0}, y_{0})(x - x_{0}) + f_{y}(x_{0}, y_{0})(y - y_{0}).
\]

But notice that \(f(x_{0},y_{0})\) is just \(z_{0}\), the height of the surface at \((x_{0}, y_{0})\). So, the tangent plane is the unique plane which most closely matches the graph of \(z = f(x, y)\) at the desired point.

\subsection{Linearizations}

Recall from Calculus I that if \(f\) is differentiable at \(x_{0}\) and \(x\) is close to \(x_{0}\) then
\[
f(x) \approx f(x_{0}) + f'(x_{0})(x - x_{0}).
\]

This is the \cyanit{linear approximation} of \(f\) at \(x_{0}\). In the same way, if \(f \colon \R^{2} \to \R\) is differentiable at \((x_{0}, y_{0})\) and \((x, y)\) is near \((x_{0}, y_{0})\), then
\[
f(x, y) \approx f(x_{0}, y_{0}) + f_{x}(x_{0}, y_{0})(x - x_{0}) + f_{y}(x_{0}, y_{0})(y - y_{0}).
\]
This is the \cyanit{linearization} of \(f\) at \((x_{0}, y_{0})\).

\subsection{Examples}

\(f(x,y) = x\cos(\pi x) \sin(\pi y)\), \((x_{0},y_{0}) = \bigl(\frac{1}{3}, \frac{1}{2}\bigr)\). Our point of interest is the pair \(\bigl(\frac{1}{3}, \frac{1}{2}\bigr)\) and \(\frac{1}{6}\), because we can just plug in the values of \(x_{0}\) and \(y_{0}\) into the function to get the height. \\

To find the equation of the tangent plane, we need to find the partial derivatives to fill out the following equation:
\[
z = \underline{\hspace{1cm}} + \underline{\hspace{1cm}}\left(x - \frac{1}{3}\right) + \underline{\hspace{1cm}}\left(y - \frac{1}{2}\right).
\]
We found \(z_{0}\) to be \(\frac{1}{6}\), and the partial derivatives are
\[
    f_{x}(x,y) = \cos(\pi x)\sin(\pi y) - \pi x\sin(\pi x)\sin(\pi y),
\]
and 
\[
    f_{x}\left(\frac{1}{3},\frac{1}{2}\right) = \frac{1}{2} - \frac{\pi}{3}\left(\frac{\mysqrt{3}}{2}\right)(1) = \frac{1}{2} - \frac{\pi\mysqrt{3}}{3}.
\]
Similarly, we have
\[
    f_{y}\left(x,y\right) = \pi x\cos(\pi x)\cos(\pi y),
\]
and
\[
    f_{y}\left(\frac{1}{3},\frac{1}{2}\right) = 0.
\]
Now, we can fill out the rest of our equation:
\[
z = \frac{1}{6} + \left(\frac{1}{2} - \frac{\pi\mysqrt{3}}{3}\right)\left(x - \frac{1}{3}\right) + 0\left(y - \frac{1}{2}\right).
\]

\newpage

\section{The Chain Rule}

\subsection{One Dimensional Chain Rule}

Suppose that \(f(x)\) and \(g(x)\) are both single variable, differentiable functions. Then, if \(h(x) = f(g(x))\), \(h\) is also differentiable and
\[
h'(x) = f'\bigl(g(x)\bigr)g'(x).
\]
Alternatively, if we write \(u = g(x)\), then
\[
h'(x) = f'(u)g'(x) \quad \text{or} \quad \dfrac{dh}{dx} = \dfrac{df}{du}\dfrac{du}{dx}.
\]
\subsection{Higher Dimensional Chain Rules}

\subsubsection{The Chain Rule – One Independent Variable}

Suppose that \(x = f(t)\), \(y = h(t)\), and \(z = f(x, y)\) are all differentiable functions. Notice that \(z\) is “really” a function of the single independent variable \(t\). Then,
\[
\dfrac{dz}{dt} = \dfrac{\partial z}{\partial x} \cdot \dfrac{dx}{dt} + \dfrac{\partial z}{\partial y} \cdot \dfrac{dy}{dt}.
\]
\subsubsection{The Chain Rule – Two Independent Variables}

Suppose that \(x = g(u, v)\), \(y = h(u, v)\), and \(z = f(x, y)\). Then,
\[
\dfrac{\partial z}{\partial u} = \dfrac{\partial z}{\partial x} \cdot \dfrac{\partial x}{\partial u} + \dfrac{\partial z}{\partial y} \cdot \dfrac{\partial y}{\partial u},
\]
and
\[
\dfrac{\partial z}{\partial v} = \dfrac{\partial z}{\partial x} \cdot \dfrac{\partial x}{\partial v} + \dfrac{\partial z}{\partial y} \cdot \dfrac{\partial y}{\partial u}.
\]
\subsubsection{The Generalized Chain Rule}

Let \(w = f(x_{1}, x_{2}, \ldots, x_{m})\) be a function of \(m\) independent variables and each \(x_{i} = x_{i}(t_{1}, t_{2}, \ldots, t_{n})\) has \(n\) independent variables. Then,
\[
\dfrac{\partial w}{\partial t_{j}} = \dfrac{\partial w}{\partial x_{1}} \cdot \dfrac{\partial x_{1}}{\partial t_{j}} + \dfrac{\partial w}{\partial x_{2}} \cdot \dfrac{\partial x_{2}}{\partial t_{j}} + \cdots + \dfrac{\partial w}{\partial x_{m}} \cdot \dfrac{\partial x_{m}}{\partial t_{j}} = \sum_{i=1}^{m} \dfrac{\partial w}{\partial x_{i}} \cdot \dfrac{\partial x_{i}}{\partial t_{j}}.
\]

\subsection{Implicit Differentiation}

Suppose we have an equation involving \(x\) and \(y\). We can rewrite this as an equation of the form \(z = f(x, y)\), where we let \(z = 0\). Then,
\[
z = 0, \text{ defines a curve for } x, y
\]
\[
\dfrac{dy}{dx} = -\dfrac{\partial f}{\partial x} \cdot \dfrac{\partial f}{\partial y}.
\]
This gives the same answer as the Calculus I \cyanit{implicit differentiation} procedure.    

\newpage

\section{Directional Derivatives and the Gradient}

The function \(z = f(x, y)\) defines a 2-dimensional surface in \(\R^{3}\). The values of \(\dfrac{\partial z}{\partial x}\) and \(\dfrac{\partial z}{\partial y}\) tell us about the rate of change of \(z\) relative to the two independent (and orthogonal) directions \(x\) and \(y\). What if we want to know about the rate of change in some other direction? \\

Let \(v\) be a vector in the direction we are interested in. Then, the \cyanit{directional derivative} at the point \((x_{1}, x_{2}, \ldots, x_{n})\) of the function \(f\) in the direction of a vector \(v\) is given by
\[
D_{v} = \dfrac{\partial f_{x_{1}}(x_{1}, x_{2}, \ldots, x_{n})}{\|v\|}v_{1} + \dfrac{\partial f_{x_{2}}(x_{1}, x_{2}, \ldots, x_{n})}{\|v\|}v_{2} + \cdots + \dfrac{\partial f_{x_{n}}(x_{1}, x_{2}, \ldots, x_{n})}{\|v\|}v_{n}.
\]
\subsection{The Gradient, \(\nabla f\)}

Suppose that \(f \colon \R^{2} \to \R\). We define the \cyanit{gradient} of \(f\), denoted by \(\nabla f(x, y)\), as
\[
\nabla f(x, y) = \dfrac{\partial f}{\partial x}(x, y)\mathbf{i} + \dfrac{\partial f}{\partial y}(x, y)\mathbf{j}.
\]
We can extend this: If \(g \colon \R^{3} \to \R\), then
\[
\nabla g(x, y, z) = \dfrac{\partial g}{\partial x}(x, y, z)\mathbf{i} + \dfrac{\partial g}{\partial y}(x, y, z)\mathbf{j} + \dfrac{\partial g}{\partial z}(x, y, z)\mathbf{k},
\]
and so on to higher dimensions. \\

The symbol \(\nabla\) is called “nabla” or “del.” Sometimes the gradient is written as \(\text{grad } f\). The gradient is a vector which points in the direction of largest increase in the value of the function – if interpreted as height, it points in the direction that is the steepest uphill. \\

Note that the \(\nabla\) lives in the domain of the function. That is, if \(f \colon \R^{2} \to \R\), then \(\nabla f \colon \R^{2} \to \R^{2}\). The gradient is a vector field – it assigns a vector to each point in the domain of the function. \\

The gradient on a contour plot is the vector which is orthogonal to the level curve at that point. This is because the gradient points in the direction of the greatest increase, and the level curve is the set of points where the function does not change.

\subsubsection{Properties of the Gradient}

\begin{itemize}
    \item If \(\nabla f(x_{0}, y_{0}) = 0\), then \(D_{u}f(x_{0}, y_{0}) = 0\) for all \(u\).
    \item If \(\nabla f(x_{0}, y_{0}) \neq 0\), then \(D_{u}f(x_{0}, y_{0})\) is maximized when \(u\) and \(\nabla f(x_{0}, y_{0})\) point in the same direction.
    \item This maximum value is \(\| \nabla f(x_{0}, y_{0}) \|\).
    \item The gradient vector is always orthogonal to any level curve.
\end{itemize}

\subsubsection{Del – Vector Definition}

We can think of \(\nabla\) as the vector
\[
\nabla = \left\langle \dfrac{\partial}{\partial x}, \dfrac{\partial}{\partial y}, \dfrac{\partial}{\partial z} \right\rangle.
\]
From chapter 2, we know that when we multiply a vector (like \(\nabla\)) by a scalar (like \(f\)), we simply multiply each coordinate of the vector by the scalar. Thus, we can think of \(\nabla f\) as a “simple” vector operation. Later, when we have functions which have vector-values themselves, we will talk about the meaning of \(\nabla \cdot \mathbf{F}\) and \(\nabla \times \mathbf{F}\). \\ 

As a final note, we will start to blur a bit the distinction between \(\R^{2}\) and \(\R^{3}\) here. We can think of always working within \(\R^{3}\), and that \(\R^{2}\) is the special case when the \(k\) component is 0.

\subsection{Example}

Let \(f(x,y) = x^{2} + y^{2} - 2x - 6y + 14\). Find the direction derivative of \(f\) at \((4,1)\), in the \(\brackett{1,3}\) direction. \\

\sol{
    We first find the partial derivative of \(f\) with respect to \(x\) and \(y\):
    \[
    f_{x}(x,y) = 2x - 2 \quad \text{and} \quad f_{y}(x,y) = 2y - 6.
    \]
    Then, at the point \((4,1)\):
    \[
    f_{x}(4,1) = 6 \quad \text{and} \quad f_{y}(4,1) = -4.
    \]
    Then, we find the unit vector in the direction of \(\brackett{1,3}\):
    \[
        \frac{\brackett{6,-4} \cdot \brackett{1,3}}{\mysqrt{10}} = \frac{-6}{\mysqrt{10}}.
    \]
}

\newpage

\section{Maxima and Minima}

For this section, we will be considering the specific case \(f \colon \R^{2} \to \R\).

\subsection{Critical Points}

The statement that \((x_{0}, y_{0})\) is a \cyanit{critical point} of \(f\) means that either:
\begin{itemize}
    \item both \(f_{x}(x_{0}, y_{0}) = 0\) and \(f_{y}(x_{0}, y_{0}) = 0\), or
    \item one or both partials does not exist.
\end{itemize}

\subsection{Maxima and Minima}

\subsubsection{Local Extrema}

The function \(f\) has a \cyanit{local maximum} at \((x_{0}, y_{0})\) provided that \(f(x_{0}, y_{0}) \geq f(x, y)\) for all choices of \((x, y)\) in some disk centered at \((x_{0}, y_{0})\) – that is, in some neighborhood of \((x_{0}, y_{0})\). \\

In a similar manner, we can define a local minimum. \\


We will use the term \cyanit{extremum} to refer to something that is either a maximum or minimum; the plural is \cyanit{extrema}. \\

Note that if there is a local extrema, \(\nabla f = \mb{0}\). This is because the gradient points in the direction of greatest increase, and if we are at a maximum or minimum, the function does not change. \\

\textbf{Fermat's Theorem for Extrema:} Suppose that \(z = f(x, y)\) and that \((x_{0}, y_{0})\) is a local extremum. Then, \((x_{0}, y_{0})\) is also a critical point.

\subsubsection{Second Derivative Test}

\textbf{Calculus I Version:} In Calculus I, the sign of the second derivative tells you whether a critical point is a local max/min, or inconclusive: Suppose that \(g \colon \R \to \R\) is a function of one variable, and \(x_{0}\) is a critical point.

\begin{itemize}
    \item if \(g''(x_{0}) > 0\), then \(x_{0}\) is a local minimum
    \item if \(g''(x_{0}) < 0\), then \(x_{0}\) is a local maximum
    \item if \(g''(x_{0}) = 0\), this test is inconclusive – it could be a max, min, or neither.
\end{itemize}

\newpage

\textbf{Multivariable Calculus Version:} We have a similar test for \(f \colon \R^{2} \to \R\), where \((x_{0}, y_{0})\) is a critical point. Define
\[
D = f_{xx}(x_{0}, y_{0})f_{yy}(x_{0}, y_{0}) - (f_{xy}\bigl(x_{0}, y_{0})\bigr)^{2}.
\]

\begin{itemize}
    \item if \(D > 0\) and \(f_{xx}(x_{0}, y_{0}) > 0\), then \(f\) has a local minimum
    \item if \(D > 0\) and \(f_{xx}(x_{0}, y_{0}) < 0\), then \(f\) has a local maximum
    \item if \(D < 0\), then \(f\) has a saddle point
    \item if \(D = 0\), then the test is inconclusive
\end{itemize}

\subsubsection{Absolute Extrema}

The function \(f\) has an \cyanit{absolute maximum} at \((x_{0}, y_{0})\) provided that \(f(x_{0}, y_{0}) \geq f(x, y)\) for all choices of \((x, y)\) in the domain of \(f\). Likewise, we can define an \cyanit{absolute minimum}. \\

\textbf{Extreme Value Theorem:} Suppose that \(f\) is continuous on a closed, bounded domain \(D\). Then \(f\) has both an absolute maximum and absolute minimum. \\

In Calculus I, we saw that a continuous function, defined on a closed interval, has its absolute extrema either at interior critical points or at the endpoints of the domain interval. This idea transfers over here – but is somewhat more complicated. \\

If \(f \colon \R^{2} \to \R\) is continuous on a closed and bounded domain, then by the Extreme Value Theorem, it has absolute extrema. These occur either at interior critical points of the domain or somewhere along the edge of the domain. The interior critical points are fairly straightforward. However, the edge of the domain is more complicated.

\subsubsection{Extrema Finding Algorithm}

\begin{itemize}
    \item Find all critical points inside the region of interest by setting the two first partials equal to zero
    \item For each boundary, use the relationship between \(x\) and \(y\) to convert \(f(x, y)\) to a new function, say \(g(x)\), of a single variable
    \item This function will have an interval \([a, b]\) as its domain
    \item Work the Calculus I problem of finding absolute extrema for \(g\) on \([a, b]\). This will generate additional points of interests.
    \begin{itemize}
        \item Absolute extrema occur at:
        \begin{enumerate}[label=\arabic*.]
            \item Interior critical points.
            \item Along the boundary.
        \end{enumerate}
    \end{itemize}
    \item Find the value of \(f(x, y)\) for each point of interest – the largest value is the absolute maximum, the smallest is the absolute minimum.
\end{itemize}

\subsection{Examples}

Let \(f(x,y) = x^{2} + y^{2} - 2x - 6y + 14\).  \\

\sol{
    We know that the critical points are \((1,3)\). We can use the second derivative test to determine that this is a local minimum.
\[
    f_{xx} = 2 \quad f_{yy} = 2 \quad f_{xy} = 0 \quad D = 4 > 0.
\]
}

Let \(f(x,y) = x^{4} + y^{4} -4xy + 1\). \\

\sol{
    Find critical points:
    \[
        f_{x} = 4x^{3} - 4y \quad f_{y} = 4y^{3} - 4x,
    \]
    and set to 0:
    \[
        x^{3} = y \quad y^{3} = x.
    \]
    Substituting, we see:
    \begin{align*}
        x^{9} - x &= 0 \\
        x(x^{8} - 1) &= 0 \\
        x(x^{4} + 1)(x^{2} + 1)(x + 1)(x - 1) &= 0. \\
        x &= 0, \pm 1 \\ 
        \therefore y &= 0, \pm 1.
    \end{align*}
    Now we have our critical points \((0,0),(1,1),(-1,-1)\). We can use the second derivative test to determine the maximum or minimums. But, to use this formula, we need the second partials for the formula:
    \[
        f_{xx} = 12x^{2} \quad f_{yy} = 12y^{2} \quad f_{xy} = -4.
    \]
    For \((0,0)\):
    \[
        0 \cdot 0 - (4)^{2} = - 16 < 0.
    \]
    Therefore, \((0,0)\) is a saddle point. For \((1,1)\):
    \[
        12 \cdot 12 - 16 = 128 > 0.
    \]
    Therefore, \((1,1)\) is a local minimum. For the same reasons, \((-1,-1)\) is also a local minimum.
}
\newpage
From Calculus I, remember we found the absolute extrema. Thus, let \(g(x) = x^{2} - 6x + 1\), on \([0,4]\). 
\sol{
    To find the absolute extrema of this function, get the critical points from the function, and also evaluate the function at the endpoints of the interval. 
    \[
        g'(x) = 2x - 6 = 0 \implies x = 3.
    \]
    Then, \(g(0) = 1\), \(g(3) = -8\), and \(g(4) = -7\). Thus, the absolute minimum is \((-8)\) and the absolute maximum is 1.
}

With that refresher, lets return to our \(f(x,y)\) function and find the absolute extrema. Specifically, find absolute extreme on the triangle with vertices \((0,0),(0,1),(\frac{1}{2},1)\). 

\sol{

}